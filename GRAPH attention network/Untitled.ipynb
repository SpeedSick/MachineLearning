{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: cora\n----- Opt. hyperparams -----\nlr: 0.005\nl2_coef: 0.0005\n----- Archi. hyperparams -----\nnb. layers: 1\nnb. units per layer: [8]\nnb. attention heads: [8, 1]\nresidual: False\nnonlinearity: <function elu at 0x7f59fcbcaae8>\nmodel: <class 'models.gat.GAT'>\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from models import GAT\n",
    "from utils import process\n",
    "\n",
    "checkpt_file = 'pre_trained/cora/mod_cora.ckpt'\n",
    "\n",
    "dataset = 'cora'\n",
    "\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 20\n",
    "patience = 100\n",
    "lr = 0.005  # learning rate\n",
    "l2_coef = 0.0005  # weight decay\n",
    "hid_units = [8] # numbers of hidden units per each attention head in each layer\n",
    "n_heads = [8, 1] # additional entry for the output layer\n",
    "residual = False\n",
    "nonlinearity = tf.nn.elu\n",
    "model = GAT\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "print('lr: ' + str(lr))\n",
    "print('l2_coef: ' + str(l2_coef))\n",
    "print('----- Archi. hyperparams -----')\n",
    "print('nb. layers: ' + str(len(hid_units)))\n",
    "print('nb. units per layer: ' + str(hid_units))\n",
    "print('nb. attention heads: ' + str(n_heads))\n",
    "print('residual: ' + str(residual))\n",
    "print('nonlinearity: ' + str(nonlinearity))\n",
    "print('model: ' + str(model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "import re\n",
    "import scipy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "train = json.loads(open('train.json').read())\n",
    "ttest = pd.read_json('test.json')\n",
    "test = json.loads(open('test.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "def get_random_data(all_data, size=2000):\n",
    "    data = all_data.copy()\n",
    "    shuffle(data)\n",
    "    return data[:size]\n",
    "def get_test_data(all_data, i=0, size=2000):\n",
    "    return all_data[i*size:(i+1)*size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7137\n20\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "cindex = 0\n",
    "all_ingredients = dict()\n",
    "all_cuisines = dict()\n",
    "rev = dict()\n",
    "for data in train + test:\n",
    "    for x in data['ingredients']:\n",
    "        if x not in all_ingredients:\n",
    "            all_ingredients[x] = index\n",
    "            index += 1\n",
    "    if data.get('cuisine') and data['cuisine'] not in all_cuisines:\n",
    "        all_cuisines[data['cuisine']] = cindex\n",
    "        rev[cindex] = data['cuisine']\n",
    "        cindex += 1\n",
    "            \n",
    "print(len(all_ingredients))\n",
    "print(len(all_cuisines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n20\n20\n20\n20\n20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n20\n20\n20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-293da6bf9d51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0mtest_cuisines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_cuisines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m \u001b[0mttest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cuisine'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_cuisines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0mttest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'cuisine'\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"submission.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0mttest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cuisine'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alan/work/Machine Learning/final/GAT/venv/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alan/work/Machine Learning/final/GAT/venv/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alan/work/Machine Learning/final/GAT/venv/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3390\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3391\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3393\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alan/work/Machine Learning/final/GAT/venv/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4000\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4001\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of values does not match length of '\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4003\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "iter_count = 5\n",
    "test_cuisines=[]\n",
    "for iter in range(iter_count):\n",
    "    nb_nodes = 2000\n",
    "    \n",
    "    data = get_random_data(train, size=nb_nodes)\n",
    "    test_data = get_test_data(test, iter, size=nb_nodes)\n",
    "    print(len(data))\n",
    "    print(len(test_data))\n",
    "    data += test_data\n",
    "    \n",
    "    nb_nodes *= 2\n",
    "    \n",
    "    adj = []\n",
    "    for i in range(nb_nodes):\n",
    "        row = []\n",
    "        for j in range(nb_nodes):\n",
    "            row.append(1 if data[i].get('cuisine') == data[j].get('cuisine') else 0)\n",
    "        adj.append(row)\n",
    "    adj = scipy.sparse.csr_matrix(adj)\n",
    "    \n",
    "    features = []\n",
    "    for i in range(nb_nodes):\n",
    "        row = [0] * len(all_ingredients)\n",
    "        for x in data[i]['ingredients']:\n",
    "            row[all_ingredients[x]] = 1\n",
    "        features.append(row)\n",
    "    features = np.matrixlib.defmatrix.matrix(features)\n",
    "    \n",
    "    y_train = []\n",
    "    y_val = []\n",
    "    for i in range(nb_nodes):\n",
    "        row = [0] * len(all_cuisines)\n",
    "        if data[i].get('cuisine'):\n",
    "            row[all_cuisines[data[i]['cuisine']]] = 1\n",
    "        y_train.append(row)\n",
    "        \n",
    "        vrow = [0] * len(all_cuisines)\n",
    "        y_val.append(vrow)\n",
    "        \n",
    "    y_train = np.matrixlib.defmatrix.matrix(y_train)\n",
    "    y_val = np.matrixlib.defmatrix.matrix(y_val)\n",
    "    y_test = np.matrixlib.defmatrix.matrix(y_val)\n",
    "    train_mask = [i < nb_nodes / 2 for i in range(nb_nodes)]\n",
    "    train_mask = np.matrixlib.defmatrix.matrix(train_mask)\n",
    "    val_mask = np.matrixlib.defmatrix.matrix([False] * nb_nodes)\n",
    "    test_mask = np.matrixlib.defmatrix.matrix([False] * nb_nodes)\n",
    "    ft_size = features.shape[1]\n",
    "    nb_classes = y_train.shape[1]\n",
    "    \n",
    "    \n",
    "\n",
    "    #features = features[np.newaxis]\n",
    "    #y_train = y_train[np.newaxis]\n",
    "    #y_val = y_val[np.newaxis]\n",
    "    #y_test = y_test[np.newaxis]\n",
    "    #train_mask = train_mask[np.newaxis]\n",
    "    #val_mask = val_mask[np.newaxis]\n",
    "    #test_mask = test_mask[np.newaxis]\n",
    "    adj = adj.todense()\n",
    "    \n",
    "    adj = adj[np.newaxis]\n",
    "    biases = process.adj_to_bias(adj, [nb_nodes], nhood=1)\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        with tf.name_scope('input'):\n",
    "            ftr_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, ft_size))\n",
    "            bias_in = tf.placeholder(dtype=tf.float32, shape=(batch_size, nb_nodes, nb_nodes))\n",
    "            lbl_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes, nb_classes))\n",
    "            msk_in = tf.placeholder(dtype=tf.int32, shape=(batch_size, nb_nodes))\n",
    "            attn_drop = tf.placeholder(dtype=tf.float32, shape=())\n",
    "            ffd_drop = tf.placeholder(dtype=tf.float32, shape=())\n",
    "            is_train = tf.placeholder(dtype=tf.bool, shape=())\n",
    "    \n",
    "        logits = model.inference(ftr_in, nb_classes, nb_nodes, is_train,\n",
    "                                    attn_drop, ffd_drop,\n",
    "                                    bias_mat=bias_in,\n",
    "                                    hid_units=hid_units, n_heads=n_heads,\n",
    "                                    residual=residual, activation=nonlinearity)\n",
    "        log_resh = tf.reshape(logits, [-1, nb_classes])\n",
    "        lab_resh = tf.reshape(lbl_in, [-1, nb_classes])\n",
    "        msk_resh = tf.reshape(msk_in, [-1])\n",
    "        loss = model.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n",
    "        accuracy = model.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "    \n",
    "        train_op = model.training(loss, lr, l2_coef)\n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    \n",
    "        vlss_mn = np.inf\n",
    "        vacc_mx = 0.0\n",
    "        curr_step = 0\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init_op)\n",
    "    \n",
    "            train_loss_avg = 0\n",
    "            train_acc_avg = 0\n",
    "            val_loss_avg = 0\n",
    "            val_acc_avg = 0\n",
    "    \n",
    "            for epoch in range(nb_epochs):\n",
    "                tr_step = 0\n",
    "                tr_size = features.shape[0]\n",
    "    \n",
    "                while tr_step * batch_size < tr_size:\n",
    "                    _, loss_value_tr, acc_tr = sess.run([train_op, loss, accuracy],\n",
    "                        feed_dict={\n",
    "                            ftr_in: features[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                            bias_in: biases[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                            lbl_in: y_train[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                            msk_in: train_mask[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                            is_train: True,\n",
    "                            attn_drop: 0.6, ffd_drop: 0.6})\n",
    "                    train_loss_avg += loss_value_tr\n",
    "                    train_acc_avg += acc_tr\n",
    "                    tr_step += 1\n",
    "    \n",
    "                vl_step = 0\n",
    "                vl_size = features.shape[0]\n",
    "    \n",
    "                while vl_step * batch_size < vl_size:\n",
    "                    loss_value_vl, acc_vl = sess.run([loss, accuracy],\n",
    "                        feed_dict={\n",
    "                            ftr_in: features[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                            bias_in: biases[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                            lbl_in: y_val[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                            msk_in: val_mask[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                            is_train: False,\n",
    "                            attn_drop: 0.0, ffd_drop: 0.0})\n",
    "                    val_loss_avg += loss_value_vl\n",
    "                    val_acc_avg += acc_vl\n",
    "                    vl_step += 1\n",
    "    \n",
    "                print('Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n",
    "                        (train_loss_avg/tr_step, train_acc_avg/tr_step,\n",
    "                        val_loss_avg/vl_step, val_acc_avg/vl_step))\n",
    "    \n",
    "                if val_acc_avg/vl_step >= vacc_mx or val_loss_avg/vl_step <= vlss_mn:\n",
    "                    if val_acc_avg/vl_step >= vacc_mx and val_loss_avg/vl_step <= vlss_mn:\n",
    "                        vacc_early_model = val_acc_avg/vl_step\n",
    "                        vlss_early_model = val_loss_avg/vl_step\n",
    "                        saver.save(sess, checkpt_file)\n",
    "                    vacc_mx = np.max((val_acc_avg/vl_step, vacc_mx))\n",
    "                    vlss_mn = np.min((val_loss_avg/vl_step, vlss_mn))\n",
    "                    curr_step = 0\n",
    "                else:\n",
    "                    curr_step += 1\n",
    "                    if curr_step == patience:\n",
    "                        print('Early stop! Min loss: ', vlss_mn, ', Max accuracy: ', vacc_mx)\n",
    "                        print('Early stop model validation loss: ', vlss_early_model, ', accuracy: ', vacc_early_model)\n",
    "                        break\n",
    "    \n",
    "                train_loss_avg = 0\n",
    "                train_acc_avg = 0\n",
    "                val_loss_avg = 0\n",
    "                val_acc_avg = 0\n",
    "    \n",
    "            saver.restore(sess, checkpt_file)\n",
    "    \n",
    "            ts_size = features.shape[0]\n",
    "            ts_step = 0\n",
    "            ts_loss = 0.0\n",
    "            ts_acc = 0.0\n",
    "    \n",
    "            while ts_step * batch_size < ts_size:\n",
    "                loss_value_ts, acc_ts = sess.run([loss, accuracy],\n",
    "                    feed_dict={\n",
    "                        ftr_in: features[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                        bias_in: biases[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                        lbl_in: y_test[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                        msk_in: test_mask[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                        is_train: False,\n",
    "                        attn_drop: 0.0, ffd_drop: 0.0})\n",
    "                ts_loss += loss_value_ts\n",
    "                ts_acc += acc_ts\n",
    "                ts_step += 1\n",
    "    \n",
    "            print('Test loss:', ts_loss/ts_step, '; Test accuracy:', ts_acc/ts_step)\n",
    "    \n",
    "            sess.close()\n",
    "    \n",
    "    fc=np.squeeze(np.asarray(y_test))\n",
    "    for i in range(nb_nodes//2,nb_nodes):\n",
    "        for j in range(20):\n",
    "            if fc[i][j]:\n",
    "                test_cuisines.append(rev[j])\n",
    "            \n",
    "test_cuisines=np.array(test_cuisines)\n",
    "ttest['cuisine']=test_cuisines\n",
    "ttest[['id' , 'cuisine' ]].to_csv(\"submission.csv\", index=False)\n",
    "ttest[['id','cuisine']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
